{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a529f2-1ed2-4b18-a2bb-a608e3b77d76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# the following is necessary on some computers:\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "#載入csv的檔案\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from packaging import version\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "\n",
    "# the following is necessary on some computers:\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ceb50-7144-47d2-9531-c480b1623003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用CountVectorizer將文字轉換成一個詞頻矩陣，表示該詞彙出現的頻率\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#建立CountVectorizer方法\n",
    "count = CountVectorizer()\n",
    "#建立矩陣\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "#bag回傳矩陣\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258194ef-84e2-4774-afbb-9ac4d54067e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count.vocabulary_)\n",
    "#印出索引值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1de3bca-bb76-4d38-9d54-bb7e9a62a7f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(bag\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bag' is not defined"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())\n",
    "#印出矩陣內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39860015-1333-4a2f-b111-1aa293a12bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "#將浮點數的顯示精度設置為小數點後兩位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb4a26-25c4-45f6-baf9-a27b078c274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算tfidf向量\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, \n",
    "                         #進行l2正規化\n",
    "                         norm='l2', \n",
    "                         smooth_idf=True)\n",
    "#印出陣列\n",
    "print(tfidf.fit_transform(count.fit_transform(docs))\n",
    "      .toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a7e4f-25cf-4b44-8b49-f994a35f2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算TF-IDF的各種值，帶入公式後，算出TF-IDF的值print出來\n",
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs+1) / (3+1))\n",
    "tfidf_is = tf_is * (idf_is + 1)\n",
    "print(f'tf-idf of term \"is\" = {tfidf_is:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce041f3-d5ba-4747-aeee-46be3d5f2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#將文檔集合轉換為 TF-IDF 矩陣\n",
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bc595-cd33-4641-81b6-945e4186d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算了最後一個文檔的 TF-IDF 值進行 l2 正規化後的結果\n",
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c501ac-1258-43f6-8b1c-18f43acf4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一個評論的最後 50 個字\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1da997-c19b-4868-a841-49e1ae805403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除特定符號，將他們值轉換成字串\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd16e2-f8c8-4e23-b031-d4b112e017e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#對第一行使用preprocessor去除符號\n",
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc66c7-9e69-4b41-b893-b010f96a1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#對輸入的資料都做preprocessor處理\n",
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5acbd-ba3e-4d78-9111-9ea55091cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#將保存review列的做preprocessor處理\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c403dc-a8fa-40a3-bbc0-f4a3ca999b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用PorterStemmer對每個單詞提取其詞幹後返回一個包含詞幹的單詞列表\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1148aa-35cc-4f48-a50d-d45c33d7b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#將輸入的文本按空格分割成單詞列表\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fb446-7a07-4a12-989a-fe007540ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#對每個單詞使用 Porter stemmer 進行詞幹提取\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc851f-460f-496a-b11c-fb614f53259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#下載停用詞\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b6697-667a-44e9-91e6-fa9011431e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#過濾掉文本中的停用詞\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
    " if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330ec85-5b25-46bb-a32b-2c3baafaa25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分為訓練集和測試集\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5b58c-842f-475e-8eb5-afd393a51f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立Pipeline，跟之前一樣\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "#使用兩個網格，各有不同的參數\n",
    "\"\"\"\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\"\"\"\n",
    "\n",
    "small_param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "                     'vect__stop_words': [None],\n",
    "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                     'clf__C': [1.0, 10.0]},\n",
    "                    {'vect__ngram_range': [(1, 1)],\n",
    "                     'vect__stop_words': [stop, None],\n",
    "                     'vect__tokenizer': [tokenizer],\n",
    "                     'vect__use_idf':[False],\n",
    "                     'vect__norm':[None],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                  'clf__C': [1.0, 10.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb07fa-56b2-48a3-b77d-f9814710e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#進行訓練\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf30e25-0065-441d-87ee-b7fe4641068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892afee1-5f89-46bd-b83c-95f3994c30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最佳模型從網格搜索 gs_lr_tfidf 中取出\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6383f4-ee27-47f6-8d1c-ff891d5a0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用了 Stratified K-Fold 交叉驗證，評估 Logistic 迴歸模型的性能\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=6)\n",
    "y = [np.random.randint(3) for i in range(25)]\n",
    "X = (y + np.random.randn(25)).reshape(-1, 1)\n",
    "\n",
    "cv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False).split(X, y))\n",
    "    \n",
    "lr = LogisticRegression()\n",
    "cross_val_score(lr, X, y, cv=cv5_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be860ce0-7317-4ec2-96d1-6a22794549c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用了 GridSearchCV 來對 Logistic 進行網格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression()\n",
    "gs = GridSearchCV(lr, {}, cv=cv5_idx, verbose=3).fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e390f1-a49f-4f67-bc78-c56f87a743d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#找最好的\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6a82f-7b7e-49ed-b6cb-88d688fefb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用了交叉驗證來評估模型的性能\n",
    "lr = LogisticRegression()\n",
    "cross_val_score(lr, X, y, cv=cv5_idx).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade01b9-373b-48fc-af41-751f0fd56b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義函式\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# The `stop` is defined as earlier in this chapter\n",
    "# Added it here for convenience, so that this section\n",
    "# can be run as standalone without executing prior code\n",
    "# in the directory\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#保留了表情符號，將文本轉換為小寫，去除了非字母、非數字、非底線的字符，過濾了停用詞\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "#逐行生成文本和對應的標籤\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de348b-fdb2-4628-83af-87e5fce974a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀CSV的數據\n",
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5069fc3-216d-4f11-aa84-69fb849c0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義get_minibatch，讀取小量的資料\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e13b7-df8f-44d3-9acb-7eeef0f513e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用HashingVectorizer 將文本數據轉換為哈希特徵向量\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ad0a0-df48-47cf-bd4b-91943fc46baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#創建了一個 SGDClassifier 分類器，並且讀取movie_data.csv\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "\n",
    "\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e7596-939b-4598-8a6e-9d51cb17ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#跟上面確認進度差不多，但有使用 HashingVectorizer 將文本轉換為特徵向量 X_train\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ed867-d041-4cd6-b684-b88a0428fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀一個5000 個文字的X_test, y_test，使用 HashingVectorizer轉換特徵向量\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707bb22f-ac98-4024-ada4-62a17d417419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#跑訓練\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46440c-9cd6-49d0-839e-850ac0471752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "\n",
    "# the following is necessary on some computers:\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe8e17-9320-4911-8a44-025dc29e664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 CountVectorizer 來將電影評論文本轉換為詞頻矩陣\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8fd403-a215-410a-9985-a2bfcd5747df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 Latent Dirichlet Allocation (LDA) 模型對詞頻矩陣 X 進行主題建模\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c853b85-5fa5-4069-a4ee-736a56f51ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861fdb0-32cb-4a24-8b48-99f7ca060c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)}:')\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae89f5-9932-40da-a2c3-2e240fe38682",
   "metadata": {},
   "outputs": [],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b2b5e-5575-45c9-aa1e-b5dc4f9c40fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
